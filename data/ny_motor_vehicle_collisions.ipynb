{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of New York Motor Vehicle Collisions\n",
    "### UCDPA Project, Hauke Laing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Source ............. https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95\n",
    "Usage tips ......... https://dev.socrata.com/foundry/data.cityofnewyork.us/h9gi-nx95\n",
    "More tips .......... https://dev.socrata.com/docs/queries/\n",
    "geospatial.......... https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "ny geomap .......... https://geopandas.org/en/stable/gallery/plotting_basemap_background.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_chunk(api, limit=1000, offset=0):\n",
    "    \"\"\"read a single chunk from the api\"\"\"\n",
    "    return pd.read_json(f\"{api}?${limit=}&${offset=}\")\n",
    "\n",
    "\n",
    "def read_api(api, total_size, chunk_size=1000):\n",
    "    \"\"\"read given number of lines from api, applying the chunk_size along the way\"\"\"\n",
    "    # use https://docs.python.org/3/reference/expressions.html#yield-expressions\n",
    "    chunk_generator = (\n",
    "        # define chunks; the last chunk might be smaller than chunk_size\n",
    "        read_api_chunk(api, limit=min(chunk_size, total_size - x), offset=x)\n",
    "        for x in range(0, total_size, chunk_size)\n",
    "    )\n",
    "    # in the generator expressions, the chunks are not yet read and stored in memory\n",
    "    # the outer paranthesis are synctactilly required for generator expressions; they\n",
    "    # are not included simply in order to permit the multiline definition\n",
    "\n",
    "    # pd.concat can handle generator expressions. According to the api reference, the objs argument\n",
    "    # accepts a sequence of DataFrame objects. This indicates that any iterable that yields DataFrame\n",
    "    # objects will be accepted, which is what chunk_generator provides.\n",
    "    return pd.concat(chunk_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input parameters\n",
    "api = \"https://data.cityofnewyork.us/resource/h9gi-nx95.json\"\n",
    "n = 50e3\n",
    "# limit = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# data_raw = read_api_chunk(api, limit=int(n))\n",
    "data_raw = read_api(api, total_size=int(100e3), chunk_size=int(25e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "borough                                  object\n",
       "collision_id                              int64\n",
       "contributing_factor_vehicle_1    string[python]\n",
       "contributing_factor_vehicle_2    string[python]\n",
       "contributing_factor_vehicle_3    string[python]\n",
       "contributing_factor_vehicle_4    string[python]\n",
       "contributing_factor_vehicle_5    string[python]\n",
       "crash_date                               object\n",
       "crash_time                       datetime64[ns]\n",
       "cross_street_name                string[python]\n",
       "latitude                                float64\n",
       "location                                 object\n",
       "longitude                               float64\n",
       "number_of_cyclist_injured                 int64\n",
       "number_of_cyclist_killed                  int64\n",
       "number_of_motorist_injured                int64\n",
       "number_of_motorist_killed                 int64\n",
       "number_of_pedestrians_injured             int64\n",
       "number_of_pedestrians_killed              int64\n",
       "number_of_persons_injured                 int64\n",
       "number_of_persons_killed                  int64\n",
       "off_street_name                  string[python]\n",
       "on_street_name                   string[python]\n",
       "vehicle_type_code_1                      object\n",
       "vehicle_type_code_2                      object\n",
       "vehicle_type_code_3                      object\n",
       "vehicle_type_code_4                      object\n",
       "vehicle_type_code_5                      object\n",
       "zip_code                                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial cleaning of data\n",
    "data_raw_2 = data_raw.rename(\n",
    "    columns={\n",
    "        \"vehicle_type_code1\": \"vehicle_type_code_1\",\n",
    "        \"vehicle_type_code2\": \"vehicle_type_code_2\",\n",
    "    }\n",
    ")\n",
    "data_raw_2 = data_raw_2.reindex(sorted(data_raw_2.columns), axis=1)\n",
    "text_cols = [col for col in data_raw_2 if re.search(\"(street|contributing_factor)\", col)]\n",
    "data_raw_2[text_cols] = data_raw_2[text_cols].astype(\"string\")\n",
    "data_raw_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index\n",
    "data_raw_2.set_index(keys=\"collision_id\", drop=False, inplace=True)\n",
    "# ensure index is unique\n",
    "data_raw_2 = data_raw_2[~data_raw_2.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plausibility check\n",
    "invalid_number_of_injured = (\n",
    "    data_raw_2[\n",
    "        [\n",
    "            \"number_of_pedestrians_injured\",\n",
    "            \"number_of_cyclist_injured\",\n",
    "            \"number_of_motorist_injured\",\n",
    "        ]\n",
    "    ].sum(axis=1)\n",
    "    > data_raw_2[\"number_of_persons_injured\"]\n",
    ")\n",
    "sum(invalid_number_of_injured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collision_id\n",
       "4455765    False\n",
       "4513547    False\n",
       "4541903    False\n",
       "4456314    False\n",
       "4486609    False\n",
       "           ...  \n",
       "4545240    False\n",
       "4521103    False\n",
       "4545395    False\n",
       "4502444     True\n",
       "4502508    False\n",
       "Name: cyclist_was_injured, Length: 99989, dtype: bool"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add column to indicate if a cyclist was injured\n",
    "data_raw_2.assign(cyclist_was_injured=lambda x: x.number_of_cyclist_injured > 0)\n",
    "data_raw_2['cyclist_was_injured']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cyclist_injured\n",
      "0    95410\n",
      "1     4508\n",
      "2       66\n",
      "3        5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# explore dataset\n",
    "occ = data_raw_2[\"number_of_cyclist_injured\"].value_counts()\n",
    "print(occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cyclist_injured\n",
      "0    0.954205\n",
      "1    0.045085\n",
      "2    0.000660\n",
      "3    0.000050\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(occ / sum(occ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "borough                                  object\n",
       "contributing_factor_vehicle      string[python]\n",
       "crash_date                               object\n",
       "crash_time                       datetime64[ns]\n",
       "cross_street_name                string[python]\n",
       "cyclist_was_injured                        bool\n",
       "latitude                                float64\n",
       "location                                 object\n",
       "longitude                               float64\n",
       "number_of_cyclist_injured                 int64\n",
       "number_of_cyclist_killed                  int64\n",
       "number_of_motorist_injured                int64\n",
       "number_of_motorist_killed                 int64\n",
       "number_of_pedestrians_injured             int64\n",
       "number_of_pedestrians_killed              int64\n",
       "number_of_persons_injured                 int64\n",
       "number_of_persons_killed                  int64\n",
       "off_street_name                  string[python]\n",
       "on_street_name                   string[python]\n",
       "vehicle_type_code                        object\n",
       "zip_code                                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset has 5 columns to cover up to 5 vehicles involved in a collision\n",
    "# format data to cover 5 vehicles in one column\n",
    "data_long = pd.wide_to_long(\n",
    "    data_raw_2,\n",
    "    stubnames=[\"vehicle_type_code_\", \"contributing_factor_vehicle_\"],\n",
    "    i=\"collision_id\",\n",
    "    j=\"vehicle_no\",\n",
    ")\n",
    "data_long.rename(\n",
    "    columns={\n",
    "        \"vehicle_type_code_\": \"vehicle_type_code\",\n",
    "        \"contributing_factor_vehicle_\": \"contributing_factor_vehicle\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "data_long = data_long.reindex(sorted(data_long.columns), axis=1)\n",
    "\n",
    "\n",
    "# keep rows for vehicle no. > 1 only if relevant information pertaining to the vehicle is present; the row is redundant otherwise\n",
    "_cnd1 = (\n",
    "    data_long[[\"vehicle_type_code\", \"contributing_factor_vehicle\"]]\n",
    "    .notnull()\n",
    "    .any(axis=1)\n",
    ")\n",
    "_cnd2 = data_long.index.get_level_values(level=1) == 1\n",
    "_cnd = _cnd1 | _cnd2\n",
    "data_long = data_long.loc[_cnd, :]\n",
    "# export long data\n",
    "# data_long.to_csv(\"data_long.csv\")\n",
    "data_long.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contributing_factor_vehicle\n",
       "Unspecified                                          104193\n",
       "Driver Inattention/Distraction                        28036\n",
       "Following Too Closely                                  7869\n",
       "Failure to Yield Right-of-Way                          7589\n",
       "Passing or Lane Usage Improper                         5268\n",
       "Unsafe Speed                                           4239\n",
       "Passing Too Closely                                    4226\n",
       "Other Vehicular                                        3980\n",
       "Traffic Control Disregarded                            3418\n",
       "Backing Unsafely                                       3285\n",
       "Turning Improperly                                     2485\n",
       "Unsafe Lane Changing                                   2351\n",
       "Driver Inexperience                                    2282\n",
       "Alcohol Involvement                                    1643\n",
       "Reaction to Uninvolved Vehicle                         1550\n",
       "Pedestrian/Bicyclist/Other Pedestrian Error/Co...      1225\n",
       "View Obstructed/Limited                                1052\n",
       "Pavement Slippery                                      1034\n",
       "Aggressive Driving/Road Rage                            835\n",
       "Oversized Vehicle                                       497\n",
       "Fell Asleep                                             453\n",
       "Brakes Defective                                        424\n",
       "Passenger Distraction                                   274\n",
       "Steering Failure                                        270\n",
       "Obstruction/Debris                                      241\n",
       "Outside Car Distraction                                 238\n",
       "Lost Consciousness                                      197\n",
       "Tire Failure/Inadequate                                 193\n",
       "Illnes                                                  174\n",
       "Pavement Defective                                      149\n",
       "Glare                                                   146\n",
       "Failure to Keep Right                                   133\n",
       "Fatigued/Drowsy                                         133\n",
       "Driverless/Runaway Vehicle                              109\n",
       "Drugs (illegal)                                          94\n",
       "Animals Action                                           88\n",
       "Accelerator Defective                                    83\n",
       "Traffic Control Device Improper/Non-Working              65\n",
       "Cell Phone (hand-Held)                                   50\n",
       "Physical Disability                                      42\n",
       "Lane Marking Improper/Inadequate                         33\n",
       "Tinted Windows                                           31\n",
       "Prescription Medication                                  19\n",
       "Vehicle Vandalism                                        16\n",
       "Other Lighting Defects                                   15\n",
       "Using On Board Navigation Device                         14\n",
       "Headlights Defective                                     11\n",
       "Tow Hitch Defective                                      11\n",
       "Other Electronic Device                                  10\n",
       "Eating or Drinking                                        8\n",
       "Cell Phone (hands-free)                                   8\n",
       "Shoulders Defective/Improper                              6\n",
       "Texting                                                   5\n",
       "Listening/Using Headphones                                4\n",
       "Windshield Inadequate                                     2\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the column 'contributing_factor_vehicle'\n",
    "data_long[\"contributing_factor_vehicle\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column 'contributing_factor_vehicle' contains a text comment. To prepare the column \n",
    "# for machine learning algorithms, we want to categorize it and later create dummies\n",
    "\n",
    "# First, establish a code representing a contributing factor and a corresponding mapping\n",
    "\n",
    "confac = (\n",
    "    data_long[\"contributing_factor_vehicle\"]\n",
    "    .drop_duplicates()\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    "    .to_frame(name=\"contributing_factor\")\n",
    ")\n",
    "\n",
    "def get_first_chars(input):\n",
    "    \"\"\"retrieve first character of each word in a string of words\"\"\"\n",
    "    return \"\".join(item[0].upper() for item in re.findall(\"\\w+\", input))\n",
    "\n",
    "# create initial code\n",
    "confac[\"cf\"] = confac[\"contributing_factor\"].apply(get_first_chars)\n",
    "\n",
    "# if code is not unique, add counting index\n",
    "confac[\"n\"] = confac.groupby([\"cf\"]).cumcount()\n",
    "_k = confac[\"n\"] > 0\n",
    "confac.loc[_k, \"cf\"] = confac.loc[_k, \"cf\"] + confac.loc[_k, \"n\"].astype(\"string\")\n",
    "confac.set_index(\"contributing_factor\", inplace=True)\n",
    "\n",
    "# export mapping for reference\n",
    "confac.to_csv(\"output/confac.csv\")\n",
    "\n",
    "confac_cols = \"cf.\" + confac[\"cf\"]\n",
    "mapping_cf = pd.Series(confac[\"cf\"]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine dummies grouped by collision_id\n",
    "# data_long[\"cf\"] = data_long[\"contributing_factor_vehicle\"].replace(mapping_cf)\n",
    "# dummies_cf_long = pd.get_dummies(data_long, columns=[\"cf\"], prefix_sep=\".\")\n",
    "# dummies_cf = dummies_cf_long[confac_cols].groupby(level=0).max()\n",
    "# dummies_cf[\"n_vehicles\"] = dummies_cf.sum(axis=1) # store number of vehicles involved\n",
    "# dummies_cf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n",
      "vehicle_type\n",
      "sedan                                  82778\n",
      "station_wagon_sport_utility_vehicle    60668\n",
      "bike                                    4948\n",
      "pick_up_truck                           3874\n",
      "box_truck                               3841\n",
      "                                       ...  \n",
      "citywide                                   1\n",
      "pro_master                                 1\n",
      "rgr                                        1\n",
      "us_postal                                  1\n",
      "c3                                         1\n",
      "Name: count, Length: 429, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# similar to the column 'contributing factor' is the column 'vehicle type'\n",
    "# we want to prepare it as categorical data\n",
    "\n",
    "# vehicle_type_codes = data_long[\"vehicle_type_code\"].astype(\"string\").dropna()\n",
    "\n",
    "# align formatting of text\n",
    "data_long[\"vehicle_type\"] = (\n",
    "    data_long[\"vehicle_type_code\"]\n",
    "    .str.replace(pat=r\"\\W+\", repl=\"_\", regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "print(data_long[\"vehicle_type\"].nunique())\n",
    "print(data_long[\"vehicle_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vt\n",
       "sedan                                  82778\n",
       "station_wagon_sport_utility_vehicle    60668\n",
       "bike                                    4948\n",
       "pick_up_truck                           3874\n",
       "box_truck                               3841\n",
       "taxi                                    3717\n",
       "bus                                     3050\n",
       "e_bike                                  2735\n",
       "motorcycle                              1715\n",
       "tractor_truck_diesel                    1431\n",
       "e_scooter                               1385\n",
       "van                                     1205\n",
       "ambulance                               1050\n",
       "other                                    869\n",
       "moped                                    686\n",
       "dump                                     628\n",
       "pk                                       427\n",
       "flat_bed                                 363\n",
       "garbage_or_refuse                        354\n",
       "convertible                              342\n",
       "carry_all                                262\n",
       "motorscooter                             240\n",
       "tow_truck_wrecker                        233\n",
       "motorbike                                218\n",
       "tractor_truck_gasoline                   195\n",
       "chassis_cab                              135\n",
       "4_dr_sedan                                97\n",
       "tanker                                    94\n",
       "fire_truck                                86\n",
       "3_door                                    59\n",
       "trailer                                   56\n",
       "limo                                      56\n",
       "refrigerated_van                          53\n",
       "concrete_mixer                            53\n",
       "unknown                                   51\n",
       "armored_truck                             47\n",
       "school_bus                                46\n",
       "flat_rack                                 43\n",
       "scooter                                   42\n",
       "unk                                       41\n",
       "multi_wheeled_vehicle                     40\n",
       "firetruck                                 37\n",
       "tow_truck                                 36\n",
       "beverage_truck                            36\n",
       "open_body                                 32\n",
       "lift_boom                                 32\n",
       "truck                                     30\n",
       "pedicab                                   26\n",
       "stake_or_rack                             26\n",
       "snow_plow                                 22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce to top 50 most frequent types of cars. last category is set to \"other\"\n",
    "top = 50\n",
    "vehicle_top_cats = data_long[\"vehicle_type\"].value_counts().head(top - 1).index\n",
    "data_long[\"vt\"] = data_long[\"vehicle_type\"]\n",
    "\n",
    "data_long.loc[\n",
    "    ~data_long[\"vehicle_type\"].isin(vehicle_top_cats)\n",
    "    & ~data_long[\"vehicle_type\"].isna(),\n",
    "    [\"vt\"],\n",
    "] = \"other\"\n",
    "\n",
    "data_long[\"vt\"] = data_long[\"vt\"].astype(\"category\")\n",
    "data_long[\"vt\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cf'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[203], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# now we are ready to create dummies from the 'contributing factor' and\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# 'vehicle type' columns. The processed versions of these columns have been \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# stored as 'cf' and 'vt'\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dummies_long \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mget_dummies(\n\u001b[0;32m      6\u001b[0m     data_long, prefix\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mvt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcf\u001b[39;49m\u001b[39m\"\u001b[39;49m], columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mvt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcf\u001b[39;49m\u001b[39m\"\u001b[39;49m], prefix_sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m dummies \u001b[39m=\u001b[39m dummies_long\u001b[39m.\u001b[39mfilter(regex\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^(vt|cf)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgroupby(level\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mmax()\n\u001b[0;32m      9\u001b[0m dummies\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:158\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput must be a list-like for parameter `columns`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     data_to_encode \u001b[39m=\u001b[39m data[columns]\n\u001b[0;32m    160\u001b[0m \u001b[39m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_len\u001b[39m(item, name):\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['cf'] not in index\""
     ]
    }
   ],
   "source": [
    "# now we are ready to create dummies from the 'contributing factor' and\n",
    "# 'vehicle type' columns. The processed versions of these columns have been \n",
    "# stored as 'cf' and 'vt'\n",
    "\n",
    "dummies_long = pd.get_dummies(\n",
    "    data_long, prefix=[\"vt\", \"cf\"], columns=[\"vt\", \"cf\"], prefix_sep=\".\"\n",
    ")\n",
    "dummies = dummies_long.filter(regex=r\"^(vt|cf)\\.\").groupby(level=0).max()\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after creating dummies, it is useful to create a copy of\n",
    "# the data with the source columns for the dummies removed\n",
    "data_raw_3 = data_raw_2.drop(\n",
    "    columns=[\n",
    "        \"vehicle_type_code_1\",\n",
    "        \"vehicle_type_code_2\",\n",
    "        \"vehicle_type_code_3\",\n",
    "        \"vehicle_type_code_4\",\n",
    "        \"vehicle_type_code_5\",\n",
    "        \"contributing_factor_vehicle_1\",\n",
    "        \"contributing_factor_vehicle_2\",\n",
    "        \"contributing_factor_vehicle_3\",\n",
    "        \"contributing_factor_vehicle_4\",\n",
    "        \"contributing_factor_vehicle_5\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions = data_raw_3.join(dummies)\n",
    "collisions.to_csv(\"output/collisions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import performance metrix\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn-metrics-confusion-matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = collisions.filter(regex=r\"^(vt|cf)\\.\", axis=1)\n",
    "y = collisions[\"cyclist_was_injured\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyclist_was_injured\n",
      "False    95410\n",
      "True      4579\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98054805 0.98089809 0.97979798 0.97984798 0.98024704]\n",
      "[0.98059806 0.98109811 0.979998   0.97979798 0.98024704]\n",
      "[0.98189819 0.98249825 0.98289829 0.98234823 0.98204731]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "dt1 = DecisionTreeClassifier(criterion='gini')\n",
    "dt2 = DecisionTreeClassifier(criterion='entropy')\n",
    "dt3 = DecisionTreeClassifier(max_depth=2)\n",
    "# dt.fit(X_train, y_train)\n",
    "# dt.score(X_test, y_test)\n",
    "print(cross_val_score(dt1, X, y))\n",
    "print(cross_val_score(dt2, X, y))\n",
    "print(cross_val_score(dt3, X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23586,   268],\n",
       "       [  209,   935]], dtype=int64)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dt = dt.predict(X_test)\n",
    "confusion_matrix(y_test, y_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.9       , 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "iris = load_iris()\n",
    "cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "cross_val_score(clf, iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71528"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test ridge classifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier\n",
    "clf_ridge = RidgeClassifier()\n",
    "clf_ridge.fit(X_train, y_train)\n",
    "y_pred = clf_ridge.predict(X_test)\n",
    "clf_ridge.score(X_test, y_test)\n",
    "# accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71464"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test random tree classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn-ensemble-randomforestclassifier\n",
    "clf_rfc = RandomForestClassifier(n_estimators=10)\n",
    "# clf_rfc = clf_rfc.fit(X_train, y_train)\n",
    "clf_rfc.fit(X_train, y_train)\n",
    "clf_rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69096"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn-neighbors-kneighborsclassifier\n",
    "clf_knn = KNeighborsClassifier(algorithm=\"ball_tree\")\n",
    "clf_knn = clf_knn.fit(X_train, y_train)\n",
    "clf_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/preprocessing-and-pipelines-4?ex=14\n",
    "# # Create models dictionary\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(),\n",
    "#     \"KNN\": KNeighborsClassifier(),\n",
    "#     \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "# }\n",
    "# results = []\n",
    "\n",
    "# # Loop through the models' values\n",
    "# for model in models.values():\n",
    "#     # Instantiate a KFold object\n",
    "#     kf = KFold(n_splits=6, random_state=12, shuffle=True)\n",
    "\n",
    "#     # Perform cross-validation\n",
    "#     cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
    "#     results.append(cv_results)\n",
    "# plt.boxplot(results, labels=models.keys())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Last step of Pipeline should implement fit or be the string 'passthrough'. '[RidgeClassifier(), RandomForestClassifier(n_estimators=10), DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9,\n                       random_state=500)]' (type <class 'list'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m make_pipeline, Pipeline\n\u001b[0;32m      3\u001b[0m pipe \u001b[39m=\u001b[39m make_pipeline([clf_ridge, clf_rfc, clf_dct])\n\u001b[1;32m----> 4\u001b[0m pipe\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:350\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps):\n\u001b[0;32m    348\u001b[0m     \u001b[39m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps)\n\u001b[1;32m--> 350\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_steps()\n\u001b[0;32m    351\u001b[0m     \u001b[39m# Setup the memory\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     memory \u001b[39m=\u001b[39m check_memory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory)\n",
      "File \u001b[1;32mc:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:247\u001b[0m, in \u001b[0;36mPipeline._validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     estimator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mand\u001b[39;00m estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLast step of Pipeline should implement fit \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor be the string \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator, \u001b[39mtype\u001b[39m(estimator))\n\u001b[0;32m    251\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Last step of Pipeline should implement fit or be the string 'passthrough'. '[RidgeClassifier(), RandomForestClassifier(n_estimators=10), DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9,\n                       random_state=500)]' (type <class 'list'>) doesn't"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "pipe = make_pipeline([clf_ridge, clf_rfc, clf_dct])\n",
    "# pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the first-layer models\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=5, min_samples_split=15, random_state=500)\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "# Create the second-layer model (meta-model)\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "# Create and fit the stacked model\n",
    "clf_stack = StackingClassifier([clf_knn, clf_dt, clf_nb], clf_lr)\n",
    "clf_stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacked model’s performance\n",
    "print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, clf_stack.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1166: RuntimeWarning: Number of classes in training fold (12) does not match total number of classes (13). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1166: RuntimeWarning: Number of classes in training fold (12) does not match total number of classes (13). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1166: RuntimeWarning: Number of classes in training fold (12) does not match total number of classes (13). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1166: RuntimeWarning: Number of classes in training fold (12) does not match total number of classes (13). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1166: RuntimeWarning: Number of classes in training fold (12) does not match total number of classes (13). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1166: RuntimeWarning: Number of classes in training fold (12) does not match total number of classes (13). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n",
      "c:\\Users\\hauke\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71872"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# # Prepare the list of tuples with the first-layer classifiers\n",
    "\n",
    "\n",
    "clf_ridge = RidgeClassifier()\n",
    "clf_rfc = RandomForestClassifier(n_estimators=10)\n",
    "clf_dct = DecisionTreeClassifier(\n",
    "    min_samples_leaf=3, min_samples_split=9, random_state=500\n",
    ")\n",
    "\n",
    "classifiers = [clf_ridge, clf_rfc, clf_dct]\n",
    "\n",
    "estimators = [\n",
    "    # ('ridge', RidgeClassifier()),\n",
    "    (\"random_forest\", RandomForestClassifier(n_estimators=10)),\n",
    "    (\n",
    "        \"decision_tree\",\n",
    "        DecisionTreeClassifier(\n",
    "            min_samples_leaf=3, min_samples_split=9, random_state=500\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Instantiate the second-layer meta estimator\n",
    "clf_meta = LogisticRegression()\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "# Build the stacking classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn-ensemble-stackingclassifier\n",
    "clf_stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=clf_meta,\n",
    "    # stack_method='predict_proba',\n",
    "    passthrough=False,\n",
    ")\n",
    "\n",
    "\n",
    "clf_stack.fit(X_train, y_train)\n",
    "clf_stack.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RidgeClassifier' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mclassifiers)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'RidgeClassifier' object is not iterable"
     ]
    }
   ],
   "source": [
    "zip(*classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc.get_params()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(vehicle_type_codes)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "def classify_cars(text):\n",
    "    if \"sedan\" in text.lower():\n",
    "        return \"Sedan\"\n",
    "    if \"truck\" in text.lower() or \"dump\" in text.lower():\n",
    "        return \"Truck\"\n",
    "    if \"sport utility vehicle\" in text.lower():\n",
    "        return \"SUV\"\n",
    "    if \"van\" in text.lower() or \"sprinter\" in text.lower():\n",
    "        return \"Van\"\n",
    "    if \"bike\" in text.lower():\n",
    "        return \"Bike\"\n",
    "    if \"taxi\" in text.lower():\n",
    "        return \"Taxi\"\n",
    "    if text in [\"Bike\", \"Motorcycle\", \"Bus\", \"E-Scooter\", \"Ambulance\"] :\n",
    "        return text\n",
    "    return \"other\"\n",
    "\n",
    "car_mapping = {\n",
    "    \"sedan\": \"Sedan\",\n",
    "    \"truck\": [\"Truck\", \"Dump\", \"Garbage or Refuse\"]\n",
    "}\n",
    "\n",
    "# data_wide[\"vehicle_type_cat\"] = data_wide[\"vehicle_type_code\"].astype(\"str\").apply(classify_cars)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols_contributing_factor = [\n",
    "    col for col in data if re.match(r\"^contributing_factor\", col)\n",
    "]\n",
    "# pd.Series(data.columns).astype(\"string\").str.startswith(\"contributing\")\n",
    "# print(cols_contributing_factor)\n",
    "# data.filter(regex=r\"^vehicle\", axis=1).isnull().sum(axis=1)\n",
    "vehicle_type_cols = data.columns.to_series().filter(regex=r\"^vehicle\").to_list()\n",
    "data[\"number_of_vehicles\"] = len(vehicle_type_cols) - data[\n",
    "    vehicle_type_cols\n",
    "].isnull().sum(axis=1)\n",
    "contributing_factor_cols = (\n",
    "    data.columns.to_series().filter(regex=r\"^contributing_factor\").to_list()\n",
    ")\n",
    "data[\"number_of_contributing_factors\"] = len(contributing_factor_cols) - data[\n",
    "    contributing_factor_cols\n",
    "].isnull().sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roads"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "on how to map categories, see here: https://stackoverflow.com/questions/62963350/how-to-categorize-a-column-with-regex-patterns\n",
    "see also here: https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"on_street_name\"].str.split(\" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injured Cyclists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"number_of_cyclist_injured\"].value_counts(dropna=True)\n",
    "# data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributing factors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# determine contributing factors\n",
    "confac_ = pd.concat(\n",
    "    (data[col] for col in data if re.match(\"contributing_factor_vehicle\", col))\n",
    ")\n",
    "confac = confac_.drop_duplicates().dropna().to_frame(name=\"ContributingFactor\")\n",
    "\n",
    "\n",
    "# get dummies\n",
    "# filter by number of involved vehicles\n",
    "# add rolling count\n",
    "# use mapping\n",
    "# extract leading characters\n",
    "def get_first_chars(input):\n",
    "    \"\"\"retrieve first character of each word\"\"\"\n",
    "    return \"\".join(item[0].upper() for item in re.findall(\"\\w+\", input))\n",
    "\n",
    "\n",
    "confac[\"CF\"] = confac[\"ContributingFactor\"].apply(get_first_chars)\n",
    "confac[\"CFCount\"] = confac.groupby([\"CF\"]).cumcount()\n",
    "# confac.loc[confac[\"MnemoCount\"] > 0, [\"Mnemo\", \"MnemoCount\"]]\n",
    "confac.loc[confac[\"CFCount\"] > 0, \"CF\"] = confac.loc[\n",
    "    confac[\"CFCount\"] > 0, \"CF\"\n",
    "] + confac.loc[confac[\"CFCount\"] > 0, \"CFCount\"].astype(\"string\")\n",
    "confac.set_index(\"ContributingFactor\", inplace=True)\n",
    "confac_mapping = pd.Series(confac[\"CF\"]).to_dict()\n",
    "\n",
    "cf_cols = [\n",
    "    re.sub(\"contributing_factor_vehicle_\", \"CFV\", col)\n",
    "    for col in contributing_factor_cols\n",
    "]\n",
    "data[cf_cols] = data[contributing_factor_cols].replace(confac_mapping)\n",
    "# pd.get_dummies(data, columns=cf_cols)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.filter(regex=r\"^CF\", axis=1)\n",
    "# pd.get_dummies(data, columns=cf_cols).filter(regex=r\"^CF\", axis=1)\n",
    "# use pd.wide_to_long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
